{
 "metadata": {
  "name": "",
  "signature": "sha256:a6ec5290b89961ae40936cc447ec10f872fbc85178972448aedb1b3a09fd861f"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Background\n",
      "\n",
      "Basically, the input of k-means algorithm is a dataset X of N points with a parameter K  indicating how many clusters are there. The output of it is a set of K cluster centroids and a labeling of X that assigns each of the points in X to a unique cluster. All points within a cluster are closer in distance to their centroid than other centroids.\n",
      "\n",
      "- Lloyd's algorithm\n",
      "    1. Once a set of centroids mu[k] is available, the clusters are updated to contain the points closest in distance to each centroid. \n",
      "    2. Given a set of clusters, the centroid are recalculated as the means of all points belonging to a cluster\n",
      "    3. Step 1 and Step 2 continues until the assignments of clusters and centroids no longer change."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Algorithm 1. KMeans"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "class KMeans:\n",
      "    def __init__(self, data, k):\n",
      "        self.data = data # data n x m\n",
      "\n",
      "        self.k = k # number of clusters k\n",
      "    \n",
      "    def _first_centroids(self):\n",
      "        # draw k points randomly from range(0, n) in data\n",
      "        centroid_index = np.random.choice(range(self.data.shape[0]), self.k, replace=False)\n",
      "        return self.data[centroid_index,:]\n",
      "    \n",
      "    def _compare_centroids(self, old_centroids, new_centroids, precision=-1):\n",
      "        if precision == -1:\n",
      "            return np.array_equal(old_centroids,new_centroids)\n",
      "            # if old = new, TURE. if old != new, FALSE\n",
      "        else:\n",
      "            # in the case that both centroids are not exactly the same, allows small difference under given precision.\n",
      "            diff = np.sum((new_centroids - old_centroids)**2, axis=1)\n",
      "            if np.max(diff) <= precision:\n",
      "                return True\n",
      "            else:\n",
      "                return False\n",
      "    \n",
      "    def _iterations(self):\n",
      "        centroids = self._first_centroids() # centroids k x m\n",
      "        \n",
      "        converged = False\n",
      "        cluster_values = []\n",
      "        iterations = 0\n",
      "        \n",
      "        while (not converged) and (iterations < 1000):\n",
      "            data_points = self.data[:, np.newaxis, :] \n",
      "            # data_points n x k x m by broadcasting\n",
      "            # To introduce the third dimension into data\n",
      "            \n",
      "            ## calculate the Euclidean distance between a centroid and a data point\n",
      "            euclidean_dist = (data_points - centroids) ** 2\n",
      "            sum_up_dist = np.sum(euclidean_dist, axis=2) # total distance over the 3rd axis (n x k)\n",
      "            \n",
      "            ## clustering, which cluster each data point belongs\n",
      "            min_dist = np.zeros(sum_up_dist.shape) # = n x k\n",
      "            min_dist[range(sum_up_dist.shape[0]), np.argmin(sum_up_dist, axis=1)] = 1\n",
      "            # [i,j] = 1 in matrix (n x k) if the ith data point belongs to cluster j\n",
      "            \n",
      "            ## clusters\n",
      "            cluster_val = np.sum(sum_up_dist[min_dist == True])\n",
      "            cluster_values.append(cluster_val)\n",
      "            \n",
      "            # new centroids\n",
      "            new_centroids = np.empty(centroids.shape)\n",
      "            for j in range(0, self.k):\n",
      "                if self.data[min_dist[:,j] == True,:].shape[0] == 0:\n",
      "                    new_centroids[j] = centroids[j]\n",
      "                else:\n",
      "                    new_centroids[j] = np.mean(self.data[min_dist[:,j] == True, :], axis=0)\n",
      "            \n",
      "            # comparing centroids\n",
      "            if self._compare_centroids(centroids, new_centroids):\n",
      "                converged = True\n",
      "            else:\n",
      "                centroids = new_centroids\n",
      "            \n",
      "            iterations += 1\n",
      "        print (iterations, 'interations are required to converge.')\n",
      "        return iterations, cluster_values, centroids, min_dist\n",
      "    \n",
      "    def cluster(self):\n",
      "        return self._iterations()\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k = 3\n",
      "data = np.random.randn(100000,2)\n",
      "kmeans = KMeans(data, k)\n",
      "_, cluster_values, centroids, min_dist = kmeans.cluster()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(197, 'interations are required to converge.')\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Algorithm 2. KMeansPlusPlus"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "class KMeansPlusPlus(KMeans):\n",
      "    def __init__(self, data, k):\n",
      "        KMeans.__init__(self, data, k)\n",
      "    \n",
      "    def _first_centroids(self):\n",
      "        # choose 1 centroid randomly from the data \n",
      "        centroids = self.data[np.random.choice(range(self.data.shape[0]),1), :]\n",
      "        data_points = self.data[:, np.newaxis, :]\n",
      "        \n",
      "        ## run k-1 passes\n",
      "        while centroids.shape[0] < self.k :\n",
      "            # the process is the same as Kmeans\n",
      "            euclidean_dist = (data_points - centroids) ** 2\n",
      "            sum_up_dist = np.sum(euclidean_dist, axis=2)\n",
      "            min_dist = np.zeros(sum_up_dist.shape)\n",
      "            min_dist[range(sum_up_dist.shape[0]), np.argmin(sum_up_dist, axis=1)] = 1\n",
      "            cluster_val = np.sum(sum_up_dist[min_dist == True])\n",
      "            \n",
      "            ## probability distribution\n",
      "            prob_distribution = np.min(sum_up_dist, axis=1)/cluster_val\n",
      "\n",
      "            ## choose the next centroid by using prob_distribution\n",
      "            centroids = np.vstack([centroids, self.data[np.random.choice(range(self.data.shape[0]),1,p=prob_distribution), :]])\n",
      "        return centroids"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k = 3\n",
      "data = np.random.randn(10000,2)\n",
      "kmeans = KMeansPlusPlus(data, k)\n",
      "_, _, centroids, min_dist = kmeans.cluster()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Algorithm 3. Scalable KMeansPlusPlus"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "class ScalableKMeansPlusPlus(KMeans):\n",
      "    def __init__(self, data, k, l, r):\n",
      "        KMeans.__init__(self, data, k)\n",
      "        self.l = l\n",
      "        self.r = r\n",
      "        \n",
      "    def _first_centroids(self):\n",
      "        centroids = self.data[np.random.choice(range(self.data.shape[0]),1), :]\n",
      "        data_points = self.data[:, np.newaxis, :]\n",
      "        \n",
      "        passes = 0\n",
      "        while passes < self.r:\n",
      "            euclidean_dist = (data_points - centroids) ** 2\n",
      "            sum_up_dist = np.sum(euclidean_dist, axis=2)\n",
      "            # the minimum distance\n",
      "            min = np.min(sum_up_dist, axis=1).reshape(-1,1)\n",
      "            # random matrix with the same size as min\n",
      "            random_matrix = np.random.rand(min.shape[0],min.shape[1])\n",
      "            # replace zeros in min with the lowest positive float\n",
      "            min[np.where(min==0)] = np.nextafter(0,1)\n",
      "            # (1.0/min)th root of random matrix\n",
      "            random_matrix = random_matrix ** (1.0/min) \n",
      "            # choose the highest l\n",
      "            center = self.data[np.argsort(random_matrix, axis=0)[:, 0]][::-1][:self.l, :]\n",
      "            # combine the new centroids with the old one\n",
      "            centroids = np.vstack((centroids, center))\n",
      "            passes += 1\n",
      "            # Finally we get a set of centroids which is higher than k\n",
      "            \n",
      "        ## reduce this to k using KMeans++\n",
      "        euclidean_dist = (data_points - centroids) ** 2\n",
      "        sum_up_dist = np.sum(euclidean_dist, axis=2)\n",
      "        min_dist = np.zeros(sum_up_dist.shape)\n",
      "        min_dist[range(sum_up_dist.shape[0]), np.argmin(sum_up_dist, axis=1)] = 1\n",
      "        weights = np.array([np.count_nonzero(min_dist[:, i]) for i in range(centroids.shape[0])],dtype='float64').reshape(-1,1)\n",
      "        prob_distribution = weights/np.sum(weights)\n",
      "        centroids = self.data[np.random.choice(range(weights.shape[0]),self.k,p=prob_distribution.ravel()),:]\n",
      "        return centroids\n",
      "        \n",
      "            \n",
      "           "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'KMeans' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-1-314ba359e6e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mScalableKMeansPlusPlus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKMeans\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mKMeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: name 'KMeans' is not defined"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "k=3\n",
      "data = np.random.randn(10000,2)\n",
      "kmeans = ScalableKMeansPlusPlus(data, k, 2, 2)\n",
      "_, _, centroids, min_location = kmeans.cluster()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'ScalableKMeansPlusPlus' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-4-e6f4e74c891a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mkmeans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mScalableKMeansPlusPlus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcentroids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_location\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcentroids\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mNameError\u001b[0m: name 'ScalableKMeansPlusPlus' is not defined"
       ]
      }
     ],
     "prompt_number": 4
    }
   ],
   "metadata": {}
  }
 ]
}