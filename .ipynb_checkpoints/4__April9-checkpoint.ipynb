{
 "metadata": {
  "name": "",
  "signature": "sha256:d5d3d39e171d0d3e8919fd68a6bf0a1543dd3340bc9e39208a29188ebb87aa93"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import timeit\n",
      "import warnings\n",
      "\n",
      "class KMeansBase:\n",
      "    def __init__(self, data, k):\n",
      "        self.data = data\n",
      "        self.k = k\n",
      "\n",
      "    def cluster(self):\n",
      "        return self._lloyds_iterations()\n",
      "\n",
      "    def _initial_centroids(self):\n",
      "        # get the initial set of centroids\n",
      "        # get k random numbers between 0 and the number of rows in the data set\n",
      "        centroid_indexes = np.random.choice(range(self.data.shape[0]), self.k, replace=False)\n",
      "        # get the corresponding data points\n",
      "        return self.data[centroid_indexes, :]\n",
      "\n",
      "    def _lloyds_iterations(self):\n",
      "        #warnings.simplefilter(\"error\")\n",
      "        centroids = self._initial_centroids()\n",
      "        #print('Initial Centroids:', centroids)\n",
      "\n",
      "        stabilized = False\n",
      "\n",
      "        j_values = []\n",
      "        iterations = 0\n",
      "        while (not stabilized) and (iterations < 1000):\n",
      "            print ('iteration counter: ', iterations)\n",
      "            try:\n",
      "                # find the Euclidean distance between a center and a data point\n",
      "                # centroids array shape = k x m\n",
      "                # data array shape = n x m\n",
      "                # In order to broadcast it, we have to introduce a third dimension into data\n",
      "                # data array becomes n x 1 x m\n",
      "                # now as a result of broadcasting, both array sizes will be n x k x m\n",
      "                data_ex = self.data[:, np.newaxis, :]\n",
      "                euclidean_dist = (data_ex - centroids) ** 2\n",
      "                # now take the summation of all distances along the 3rd axis(length of the dimension is m).\n",
      "                # This will be the total distance from each centroid for each data point.\n",
      "                # resulting vector will be of size n x k\n",
      "                distance_arr = np.sum(euclidean_dist, axis=2)\n",
      "\n",
      "                # now we need to find out to which cluster each data point belongs.\n",
      "                # Use a matrix of n x k where [i,j] = 1 if the ith data point belongs\n",
      "                # to cluster j.\n",
      "                min_location = np.zeros(distance_arr.shape)\n",
      "                min_location[range(distance_arr.shape[0]), np.argmin(distance_arr, axis=1)] = 1\n",
      "\n",
      "                # calculate J\n",
      "                j_val = np.sum(distance_arr[min_location == True])\n",
      "                j_values.append(j_val)\n",
      "\n",
      "                # calculates the new centroids\n",
      "                new_centroids = np.empty(centroids.shape)\n",
      "                for col in range(0, self.k):\n",
      "                    if self.data[min_location[:, col] == True,:].shape[0] == 0:\n",
      "                        new_centroids[col] = centroids[col]\n",
      "                    else:\n",
      "                        new_centroids[col] = np.mean(self.data[min_location[:, col] == True, :], axis=0)\n",
      "\n",
      "                # compare centroids to see if they are equal or not\n",
      "                if self._compare_centroids(centroids, new_centroids):\n",
      "                    # it has resulted in the same centroids.\n",
      "                    stabilized = True\n",
      "                else:\n",
      "                    centroids = new_centroids\n",
      "            except:\n",
      "                print ('exception!')\n",
      "                continue\n",
      "            else:\n",
      "                iterations += 1\n",
      "\n",
      "        print ('Required ', iterations, ' iterations to stabilize.')\n",
      "        return iterations, j_values, centroids, min_location\n",
      "\n",
      "    def _compare_centroids(self, old_centroids, new_centroids, precision=-1):\n",
      "        if precision == -1:\n",
      "            return np.array_equal(old_centroids, new_centroids)\n",
      "        else:\n",
      "            diff = np.sum((new_centroids - old_centroids)**2, axis=1)\n",
      "            if np.max(diff) <= precision:\n",
      "                return True\n",
      "            else:\n",
      "                return False\n",
      "\n",
      "    def initCost(self):\n",
      "        t = timeit.Timer(lambda: self._initial_centroids())\n",
      "        return t.timeit(number=10)\n",
      "    \n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "class KMeansPP(KMeansBase):\n",
      "    def __init__(self, data, k):\n",
      "        KMeansBase.__init__(self, data, k)\n",
      "\n",
      "    def _initial_centroids(self):\n",
      "        # random initial centroid \n",
      "        centroids = self.data[np.random.choice(range(self.data.shape[0]),1), :]\n",
      "        data_ex = self.data[:, np.newaxis, :]\n",
      "\n",
      "        # k - 1 passes to select the initial centroids\n",
      "        while centroids.shape[0] < self.k :\n",
      "            #print (centroids)\n",
      "            euclidean_dist = (data_ex - centroids) ** 2\n",
      "            distance_arr = np.sum(euclidean_dist, axis=2)\n",
      "            min_location = np.zeros(distance_arr.shape)\n",
      "            min_location[range(distance_arr.shape[0]), np.argmin(distance_arr, axis=1)] = 1\n",
      "            # calculate J\n",
      "            j_val = np.sum(distance_arr[min_location == True])\n",
      "            # calculate the probability distribution\n",
      "            prob_dist = np.min(distance_arr, axis=1)/j_val\n",
      "            # select the next centroid using the probability distribution calculated before\n",
      "            centroids = np.vstack([centroids, self.data[np.random.choice(range(self.data.shape[0]),1, p = prob_dist), :]])\n",
      "        return centroids"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class ScalableKMeansPP(KMeansBase):\n",
      "    def __init__(self, data, k, l, r):\n",
      "        KMeansBase.__init__(self, data, k)\n",
      "        self.l = l\n",
      "        self.r = r\n",
      "\n",
      "    def _initial_centroids(self):\n",
      "        # random initial centroid \n",
      "        centroids = self.data[np.random.choice(range(self.data.shape[0]),1), :]\n",
      "        data_ex = self.data[:, np.newaxis, :]\n",
      "\n",
      "        passes = 0\n",
      "        while passes < self.r:\n",
      "            euclidean_dist = (data_ex - centroids) ** 2\n",
      "            distance_arr = np.sum(euclidean_dist, axis=2)\n",
      "            # find the minimum distance\n",
      "            min = np.min(distance_arr, axis=1).reshape(-1, 1)\n",
      "            # let's use weighted sampling algorithm to select l centroids\n",
      "            random_numbers = np.random.rand(min.shape[0], min.shape[1])\n",
      "            # replace zeros in min if available with the lowest positive float in Python\n",
      "            min[np.where(min==0)] = np.nextafter(0,1)\n",
      "            # take the n^th root of random numbers where n is the weights\n",
      "            with np.errstate(all='ignore'):\n",
      "                random_numbers = random_numbers ** (1.0/min)\n",
      "            # pick the highest l\n",
      "            cent = self.data[np.argsort(random_numbers, axis=0)[:, 0]][::-1][:self.l, :]\n",
      "            # combine the new set of centroids with the previous set\n",
      "            centroids = np.vstack((centroids, cent))\n",
      "            passes += 1\n",
      "        # now we have the initial set of centroids which is higher than k.\n",
      "        # we should reduce this to k using scalable K-Means++\n",
      "        euclidean_dist = (data_ex - centroids) ** 2\n",
      "        distance_arr = np.sum(euclidean_dist, axis=2)\n",
      "        min_location = np.zeros(distance_arr.shape)\n",
      "        min_location[range(distance_arr.shape[0]), np.argmin(distance_arr, axis=1)] = 1\n",
      "        weights = np.array([np.count_nonzero(min_location[:, col]) for col in range(centroids.shape[0])]).reshape(-1,1)\n",
      "        # cluster these r*l + 1 points with K-Means++ to get K points\n",
      "        kmeans_pp = KMeansPP(weights, self.k)\n",
      "        _, _, _, min_locations = kmeans_pp.cluster()\n",
      "        # calculates the new centroids\n",
      "        new_centroids = np.empty((self.k, self.data.shape[1]))\n",
      "        for col in range(0, self.k):\n",
      "            new_centroids[col] = np.mean(centroids[min_locations[:, col] == True, :], axis=0)\n",
      "        return new_centroids\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}