
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass{article}

    
    
    \usepackage{graphicx} % Used to insert images
    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{color} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    

    
    
    \definecolor{orange}{cmyk}{0,0.4,0.8,0.2}
    \definecolor{darkorange}{rgb}{.71,0.21,0.01}
    \definecolor{darkgreen}{rgb}{.12,.54,.11}
    \definecolor{myteal}{rgb}{.26, .44, .56}
    \definecolor{gray}{gray}{0.45}
    \definecolor{lightgray}{gray}{.95}
    \definecolor{mediumgray}{gray}{.8}
    \definecolor{inputbackground}{rgb}{.95, .95, .85}
    \definecolor{outputbackground}{rgb}{.95, .95, .95}
    \definecolor{traceback}{rgb}{1, .95, .95}
    % ansi colors
    \definecolor{red}{rgb}{.6,0,0}
    \definecolor{green}{rgb}{0,.65,0}
    \definecolor{brown}{rgb}{0.6,0.6,0}
    \definecolor{blue}{rgb}{0,.145,.698}
    \definecolor{purple}{rgb}{.698,.145,.698}
    \definecolor{cyan}{rgb}{0,.698,.698}
    \definecolor{lightgray}{gray}{0.5}
    
    % bright ansi colors
    \definecolor{darkgray}{gray}{0.25}
    \definecolor{lightred}{rgb}{1.0,0.39,0.28}
    \definecolor{lightgreen}{rgb}{0.48,0.99,0.0}
    \definecolor{lightblue}{rgb}{0.53,0.81,0.92}
    \definecolor{lightpurple}{rgb}{0.87,0.63,0.87}
    \definecolor{lightcyan}{rgb}{0.5,1.0,0.83}
    
    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{MINJUNG\_PARK\_FINAL\_PROJECT}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=blue,
      linkcolor=darkorange,
      citecolor=darkgreen,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{k+kn}{import} \PY{n+nn}{os}
        \PY{k+kn}{import} \PY{n+nn}{sys}
        \PY{k+kn}{import} \PY{n+nn}{glob}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib.pyplot} \PY{k+kn}{as} \PY{n+nn}{plt}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib.cm} \PY{k+kn}{as} \PY{n+nn}{cm}
        \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k+kn}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k+kn}{as} \PY{n+nn}{pd}
        \PY{o}{\PYZpc{}}\PY{k}{matplotlib} \PY{n}{inline}
        \PY{o}{\PYZpc{}}\PY{k}{precision} \PY{l+m+mi}{4}
        \PY{n}{plt}\PY{o}{.}\PY{n}{style}\PY{o}{.}\PY{n}{use}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{ggplot}\PY{l+s}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}

    \section{\textbf{STA 663 Project
Outline}}\label{sta-663-project-outline}

\subsection{\emph{Background Clustering}}\label{background-clustering}

Clustering is to assign a set of data points into clusters. It can be
applied into the real life example. For instance, imagine you go to the
IKEA. Items in the IKEA are usually well organized to be easily found.
There are several sections we can choose; living room, Bedroom, Bathroom
and so on. Also, a Bedroom part has some aspects like bedding, storage,
etc. If we can quantify this qualitative data, these aspects for the
Bedroom can be clustered into one categories. Clustering method can be
useful in this case.

\subsection{\emph{Scalable K-means++}}\label{scalable-k-means}

Bahman Bahmani, Benjamin Moseley, Andrea Vattani, Ravi Kumar and Sergei
Vassilvitskii

Clustering is one of the most important issues in data mining.
Especially, K-means is considered as the most popular clustering method.
K-means has an advantage on its simplicity, just starting with randomly
chosen initial centers, repeatedly assigning each input point to its
nearest center, and then recalculating the centers given the assigned
points. However, K-means also has a disadvantage on its time efficiency
and quality. In this paper, they describe a pallel version of K-means++
initialization algorithm and prove its efficiency. Also, the main idea
of their paper is that they want to implement sampling O(k) points in
each round and repeat the process for O(log n) rounds rather than just
sampling a single point in each pass of the k-mean++ algorithm. At the
end of the algorithm, they are left with O(klogn) points that form a
solution that is within a constant factor away from the optimum and then
recluster these points into k initial centers for the Lloyd's iteration.
They called this initialization algorithm k-means\textbar{}\textbar{}

k-means\textbar{}\textbar{} has several advantages. First, O(log n)
iterations are not necessary. k-means\textbar{}\textbar{} can find
better solution than any other meathod. Second,
k-means\textbar{}\textbar{} is much faster than existing parallel
algorithms for k-means. Third, the number of interations can be
smallest.

    \subsection{\emph{Outline of algorithm}}\label{outline-of-algorithm}

\subsubsection{\textbf{Algorithm 1, KMeans}}\label{algorithm-1-kmeans}

Basically, the input of k-means algorithm is a dataset X (vectorized) of
N points with a parameter K indicating how many clusters are there. The
output of it is a set of K cluster centroids and a labeling of X that
assigns each of the points in X to a unique cluster. All points within a
cluster are closer in distance to their centroid than other centroids.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{k}{def} \PY{n+nf}{KMeans}\PY{p}{(}\PY{n}{data}\PY{p}{,}\PY{n}{centroids}\PY{p}{,}\PY{n}{k}\PY{p}{)}\PY{p}{:} 
                
            \PY{n}{converged} \PY{o}{=} \PY{n+nb+bp}{False}
            \PY{n}{cluster\PYZus{}values} \PY{o}{=} \PY{p}{[}\PY{p}{]}
            \PY{n}{iterations} \PY{o}{=} \PY{l+m+mi}{0}
                
            \PY{k}{while} \PY{p}{(}\PY{o+ow}{not} \PY{n}{converged}\PY{p}{)} \PY{o+ow}{and} \PY{p}{(}\PY{n}{iterations} \PY{o}{\PYZlt{}} \PY{l+m+mi}{1000}\PY{p}{)}\PY{p}{:}
                \PY{n}{data\PYZus{}points} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{newaxis}\PY{p}{,} \PY{p}{:}\PY{p}{]} 
                \PY{c}{\PYZsh{} data\PYZus{}points n x k x m by broadcasting}
                \PY{c}{\PYZsh{} To introduce the third dimension into data}
                
                \PY{c}{\PYZsh{}\PYZsh{} calculate the Euclidean distance between a centroid and a data point}
                \PY{n}{euclidean\PYZus{}dist} \PY{o}{=} \PY{p}{(}\PY{n}{data\PYZus{}points} \PY{o}{\PYZhy{}} \PY{n}{centroids}\PY{p}{)} \PY{o}{*}\PY{o}{*} \PY{l+m+mi}{2}
                \PY{n}{sum\PYZus{}up\PYZus{}dist} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{euclidean\PYZus{}dist}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)} \PY{c}{\PYZsh{} total distance over the 3rd axis (n x k)}
                
                \PY{c}{\PYZsh{}\PYZsh{} clustering, which cluster each data point belongs}
                \PY{n}{min\PYZus{}dist} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{sum\PYZus{}up\PYZus{}dist}\PY{o}{.}\PY{n}{shape}\PY{p}{)} \PY{c}{\PYZsh{} = n x k}
                \PY{n}{min\PYZus{}dist}\PY{p}{[}\PY{n+nb}{range}\PY{p}{(}\PY{n}{sum\PYZus{}up\PYZus{}dist}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{argmin}\PY{p}{(}\PY{n}{sum\PYZus{}up\PYZus{}dist}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{1}
                \PY{c}{\PYZsh{} [i,j] = 1 in matrix (n x k) if the ith data point belongs to cluster j}
                
                \PY{c}{\PYZsh{}\PYZsh{} clusters}
                \PY{n}{cluster\PYZus{}val} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{sum\PYZus{}up\PYZus{}dist}\PY{p}{[}\PY{n}{min\PYZus{}dist} \PY{o}{==} \PY{n+nb+bp}{True}\PY{p}{]}\PY{p}{)}
                \PY{n}{cluster\PYZus{}values}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{cluster\PYZus{}val}\PY{p}{)}
                    
                \PY{c}{\PYZsh{} new centroids}
                \PY{n}{new\PYZus{}centroids} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{empty}\PY{p}{(}\PY{n}{centroids}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
                \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{k}\PY{p}{)}\PY{p}{:}
                    \PY{k}{if} \PY{n}{data}\PY{p}{[}\PY{n}{min\PYZus{}dist}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{j}\PY{p}{]} \PY{o}{==} \PY{n+nb+bp}{True}\PY{p}{,}\PY{p}{:}\PY{p}{]}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                        \PY{n}{new\PYZus{}centroids}\PY{p}{[}\PY{n}{j}\PY{p}{]} \PY{o}{=} \PY{n}{centroids}\PY{p}{[}\PY{n}{j}\PY{p}{]}
                    \PY{k}{else}\PY{p}{:}
                        \PY{n}{new\PYZus{}centroids}\PY{p}{[}\PY{n}{j}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{n}{min\PYZus{}dist}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{j}\PY{p}{]} \PY{o}{==} \PY{n+nb+bp}{True}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
                \PY{c}{\PYZsh{} comparing centroids}
                \PY{k}{if} \PY{n}{np}\PY{o}{.}\PY{n}{array\PYZus{}equal}\PY{p}{(}\PY{n}{centroids}\PY{p}{,}\PY{n}{new\PYZus{}centroids}\PY{p}{)}\PY{p}{:}
                    \PY{n}{converged} \PY{o}{=} \PY{n+nb+bp}{True}
                \PY{k}{else}\PY{p}{:}
                    \PY{n}{centroids} \PY{o}{=} \PY{n}{new\PYZus{}centroids}
                    
                \PY{n}{iterations} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
            \PY{k}{print} \PY{p}{(}\PY{n}{iterations}\PY{p}{,} \PY{l+s}{\PYZsq{}}\PY{l+s}{iterations are required to converge.}\PY{l+s}{\PYZsq{}}\PY{p}{)}
            \PY{k}{return} \PY{n}{iterations}\PY{p}{,} \PY{n}{cluster\PYZus{}values}\PY{p}{,} \PY{n}{centroids}\PY{p}{,} \PY{n}{min\PYZus{}dist}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{c}{\PYZsh{}\PYZsh{} Unit testing}
         
         \PY{n}{k} \PY{o}{=} \PY{l+m+mi}{3}
         \PY{c}{\PYZsh{} if all of inputs is correct, it works well.}
         \PY{n}{data1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{l+m+mi}{1000}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}
         \PY{n}{centroids1} \PY{o}{=} \PY{n}{data1}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{choice}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n}{data1}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{k}\PY{p}{,} \PY{n}{replace}\PY{o}{=}\PY{n+nb+bp}{False}\PY{p}{)}\PY{p}{,}\PY{p}{:}\PY{p}{]}
         \PY{k}{print} \PY{n}{KMeans}\PY{p}{(}\PY{n}{data1}\PY{p}{,}\PY{n}{centroids1}\PY{p}{,}\PY{n}{k}\PY{p}{)}
         
         \PY{c}{\PYZsh{} if the input is an empty vector, it gives an error.}
         \PY{n}{data2} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{centroids2} \PY{o}{=} \PY{n}{data1}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{choice}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n}{data1}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{k}\PY{p}{,} \PY{n}{replace}\PY{o}{=}\PY{n+nb+bp}{False}\PY{p}{)}\PY{p}{,}\PY{p}{:}\PY{p}{]}
         \PY{n}{KMeans}\PY{p}{(}\PY{n}{data2}\PY{p}{,}\PY{n}{centroids2}\PY{p}{,}\PY{n}{k}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
(11, 'iterations are required to converge.')
(11, [1362.1419180869309, 1118.6938971038817, 990.53102606943298, 940.17957292690141, 928.64624932636059, 925.92729649578928, 924.95467235173282, 924.42366796063197, 924.16217561580538, 924.0817154214501, 924.07612185130051], array([[-0.2948, -1.0542],
       [-0.7074,  0.7313],
       [ 1.0151,  0.1881]]), array([[ 1.,  0.,  0.],
       [ 0.,  0.,  1.],
       [ 1.,  0.,  0.],
       \ldots, 
       [ 0.,  1.,  0.],
       [ 0.,  1.,  0.],
       [ 1.,  0.,  0.]]))
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]

        ---------------------------------------------------------------------------
    TypeError                                 Traceback (most recent call last)

        <ipython-input-14-fcf3d672d7b9> in <module>()
          9 data2 = []
         10 centroids2 = data1[np.random.choice(range(data1.shape[0]), k, replace=False),:]
    ---> 11 KMeans(data2,centroids2,k)
    

        <ipython-input-2-25d79bca730b> in KMeans(data, centroids, k)
          6 
          7     while (not converged) and (iterations < 1000):
    ----> 8         data\_points = data[:, np.newaxis, :]
          9         \# data\_points n x k x m by broadcasting
         10         \# To introduce the third dimension into data


        TypeError: list indices must be integers, not tuple

    \end{Verbatim}

    \subsubsection{\textbf{Algorithm 2, KMeans++}}\label{algorithm-2-kmeans}

This is the algorithm to get initial centroids points. Firstly, choose 1
centroid randomly from the data. Then, use it to generate the next
centroid until getting k number of centroids with the probablility
distribution. Finally, this algorithm gives k centroids which will be
used as the initial centroids for KMeans.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{k}{def} \PY{n+nf}{KMeansPlusPlus}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{k}\PY{p}{)}\PY{p}{:}
             \PY{c}{\PYZsh{} choose 1 centroid randomly from the data }
             \PY{n}{centroids} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{choice}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n}{data}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{p}{:}\PY{p}{]}
             \PY{n}{data\PYZus{}points} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{newaxis}\PY{p}{,} \PY{p}{:}\PY{p}{]}
             
             \PY{c}{\PYZsh{}\PYZsh{} run k\PYZhy{}1 passes}
             \PY{k}{while} \PY{n}{centroids}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]} \PY{o}{\PYZlt{}} \PY{n}{k} \PY{p}{:}
                 \PY{c}{\PYZsh{} the process is the same as Kmeans}
                 \PY{n}{euclidean\PYZus{}dist} \PY{o}{=} \PY{p}{(}\PY{n}{data\PYZus{}points} \PY{o}{\PYZhy{}} \PY{n}{centroids}\PY{p}{)} \PY{o}{*}\PY{o}{*} \PY{l+m+mi}{2}
                 \PY{n}{sum\PYZus{}up\PYZus{}dist} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{euclidean\PYZus{}dist}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
                 \PY{n}{min\PYZus{}dist} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{sum\PYZus{}up\PYZus{}dist}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
                 \PY{n}{min\PYZus{}dist}\PY{p}{[}\PY{n+nb}{range}\PY{p}{(}\PY{n}{sum\PYZus{}up\PYZus{}dist}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{argmin}\PY{p}{(}\PY{n}{sum\PYZus{}up\PYZus{}dist}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{1}
                 \PY{n}{cluster\PYZus{}val} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{sum\PYZus{}up\PYZus{}dist}\PY{p}{[}\PY{n}{min\PYZus{}dist} \PY{o}{==} \PY{n+nb+bp}{True}\PY{p}{]}\PY{p}{)}
                     
                 \PY{c}{\PYZsh{}\PYZsh{} probability distribution}
                 \PY{n}{prob\PYZus{}distribution} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{n}{sum\PYZus{}up\PYZus{}dist}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{/}\PY{n}{cluster\PYZus{}val}
         
                 \PY{c}{\PYZsh{}\PYZsh{} choose the next centroid by using prob\PYZus{}distribution}
                 \PY{n}{centroids} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{vstack}\PY{p}{(}\PY{p}{[}\PY{n}{centroids}\PY{p}{,} \PY{n}{data}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{choice}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n}{data}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{p}\PY{o}{=}\PY{n}{prob\PYZus{}distribution}\PY{p}{)}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{p}{]}\PY{p}{)}
             \PY{k}{return} \PY{n}{centroids}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{c}{\PYZsh{}\PYZsh{} Unit testing}
         
         \PY{n}{k} \PY{o}{=} \PY{l+m+mi}{3}
         \PY{c}{\PYZsh{} if all of inputs is correct, it works well.}
         \PY{n}{data1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{l+m+mi}{1000}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}
         \PY{k}{print} \PY{n}{KMeansPlusPlus}\PY{p}{(}\PY{n}{data1}\PY{p}{,}\PY{n}{k}\PY{p}{)}
         
         \PY{c}{\PYZsh{} if the input is an empty vector, it gives an error.}
         \PY{n}{data2} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{KMeansPlusPlus}\PY{p}{(}\PY{n}{data2}\PY{p}{,}\PY{n}{k}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
[[-0.4407 -0.3963]
 [ 0.966   1.1351]
 [ 0.4302 -0.0434]]
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]

        ---------------------------------------------------------------------------
    AttributeError                            Traceback (most recent call last)

        <ipython-input-18-74447a50088c> in <module>()
          8 \# if the input is an empty vector, it gives an error.
          9 data2 = []
    ---> 10 KMeansPlusPlus(data2,k)
    

        <ipython-input-16-fca45e9de50b> in KMeansPlusPlus(data, k)
          1 def KMeansPlusPlus(data, k):
          2     \# choose 1 centroid randomly from the data
    ----> 3     centroids = data[np.random.choice(range(data.shape[0]),1), :]
          4     data\_points = data[:, np.newaxis, :]
          5 


        AttributeError: 'list' object has no attribute 'shape'

    \end{Verbatim}

    \subsubsection{\textbf{Algorithm 3, Scalable
KMeans++}}\label{algorithm-3-scalable-kmeans}

This is also the algorithm to get initial centroids points. Firstly,
choose 1 centroid randomly from the data. Then, use it to generate the
next centroid until getting a set of centroids which is more than k
numbers with the probablility distribution made by the weights. After
that, reduce this set of centroids which is higher than k into k numbers
of centroids by using KMeans++. Finally, this algorithm gives k
centroids which will be used as the initial centroids for KMeans.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{k}{def} \PY{n+nf}{ScalableKMeansPlusPlus}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{k}\PY{p}{,} \PY{n}{l}\PY{p}{,} \PY{n}{r}\PY{p}{)}\PY{p}{:}
             \PY{n}{centroids} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{choice}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n}{data}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{,} \PY{p}{:}\PY{p}{]}
             \PY{n}{data\PYZus{}points} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{newaxis}\PY{p}{,} \PY{p}{:}\PY{p}{]}
             \PY{n}{passes} \PY{o}{=} \PY{l+m+mi}{0}
             
             \PY{k}{while} \PY{n}{passes} \PY{o}{\PYZlt{}} \PY{n}{r}\PY{p}{:}
                 \PY{n}{euclidean\PYZus{}dist} \PY{o}{=} \PY{p}{(}\PY{n}{data\PYZus{}points} \PY{o}{\PYZhy{}} \PY{n}{centroids}\PY{p}{)} \PY{o}{*}\PY{o}{*} \PY{l+m+mi}{2}
                 \PY{n}{sum\PYZus{}up\PYZus{}dist} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{euclidean\PYZus{}dist}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
                 \PY{c}{\PYZsh{} the minimum distance}
                 \PY{n+nb}{min} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{min}\PY{p}{(}\PY{n}{sum\PYZus{}up\PYZus{}dist}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
                 \PY{c}{\PYZsh{} random matrix with the same size as min}
                 \PY{n}{random\PYZus{}matrix} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{rand}\PY{p}{(}\PY{n+nb}{min}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n+nb}{min}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
                 \PY{c}{\PYZsh{} replace zeros in min with the lowest positive float}
                 \PY{n+nb}{min}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{where}\PY{p}{(}\PY{n+nb}{min}\PY{o}{==}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{nextafter}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
                 \PY{c}{\PYZsh{} (1.0/min)th root of random matrix}
                 \PY{n}{random\PYZus{}matrix} \PY{o}{=} \PY{n}{random\PYZus{}matrix} \PY{o}{*}\PY{o}{*} \PY{p}{(}\PY{l+m+mf}{1.0}\PY{o}{/}\PY{n+nb}{min}\PY{p}{)} 
                 \PY{c}{\PYZsh{} choose the highest l}
                 \PY{n}{center} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argsort}\PY{p}{(}\PY{n}{random\PYZus{}matrix}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{[}\PY{p}{:}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{p}{:}\PY{n}{l}\PY{p}{,} \PY{p}{:}\PY{p}{]}
                 \PY{c}{\PYZsh{} combine the new centroids with the old one}
                 \PY{n}{centroids} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{vstack}\PY{p}{(}\PY{p}{(}\PY{n}{centroids}\PY{p}{,} \PY{n}{center}\PY{p}{)}\PY{p}{)}
                 \PY{n}{passes} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
                 \PY{c}{\PYZsh{} Finally we get a set of centroids which is higher than k}
             \PY{c}{\PYZsh{}\PYZsh{} reduce this to k using KMeans++}
             \PY{n}{euclidean\PYZus{}dist} \PY{o}{=} \PY{p}{(}\PY{n}{data\PYZus{}points} \PY{o}{\PYZhy{}} \PY{n}{centroids}\PY{p}{)} \PY{o}{*}\PY{o}{*} \PY{l+m+mi}{2}
             \PY{n}{sum\PYZus{}up\PYZus{}dist} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{euclidean\PYZus{}dist}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
             \PY{n}{min\PYZus{}dist} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{sum\PYZus{}up\PYZus{}dist}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
             \PY{n}{min\PYZus{}dist}\PY{p}{[}\PY{n+nb}{range}\PY{p}{(}\PY{n}{sum\PYZus{}up\PYZus{}dist}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{argmin}\PY{p}{(}\PY{n}{sum\PYZus{}up\PYZus{}dist}\PY{p}{,} \PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{]} \PY{o}{=} \PY{l+m+mi}{1}
             \PY{n}{weights} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{count\PYZus{}nonzero}\PY{p}{(}\PY{n}{min\PYZus{}dist}\PY{p}{[}\PY{p}{:}\PY{p}{,} \PY{n}{i}\PY{p}{]}\PY{p}{)} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{centroids}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{]}\PY{p}{,}\PY{n}{dtype}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{float64}\PY{l+s}{\PYZsq{}}\PY{p}{)}\PY{o}{.}\PY{n}{reshape}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{)}
             \PY{n}{prob\PYZus{}distribution} \PY{o}{=} \PY{n}{weights}\PY{o}{/}\PY{n}{np}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{n}{weights}\PY{p}{)}
             \PY{n}{centroids} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{choice}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n}{weights}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{,}\PY{n}{k}\PY{p}{,}\PY{n}{p}\PY{o}{=}\PY{n}{prob\PYZus{}distribution}\PY{o}{.}\PY{n}{ravel}\PY{p}{(}\PY{p}{)}\PY{p}{)}\PY{p}{,}\PY{p}{:}\PY{p}{]}
             \PY{k}{return} \PY{n}{centroids}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} \PY{c}{\PYZsh{}\PYZsh{} Unit testing}
         
         \PY{n}{k} \PY{o}{=} \PY{l+m+mi}{3}
         \PY{n}{l} \PY{o}{=} \PY{l+m+mi}{2}
         \PY{n}{r} \PY{o}{=} \PY{l+m+mi}{2}
         \PY{c}{\PYZsh{} if all of inputs is correct, it works well.}
         \PY{n}{data1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randn}\PY{p}{(}\PY{l+m+mi}{1000}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}
         \PY{k}{print} \PY{n}{ScalableKMeansPlusPlus}\PY{p}{(}\PY{n}{data1}\PY{p}{,}\PY{n}{k}\PY{p}{,}\PY{n}{l}\PY{p}{,}\PY{n}{r}\PY{p}{)}
         
         \PY{c}{\PYZsh{} if the input is an empty vector, it gives an error.}
         \PY{n}{data2} \PY{o}{=} \PY{p}{[}\PY{p}{]}
         \PY{n}{ScalableKMeansPlusPlus}\PY{p}{(}\PY{n}{data2}\PY{p}{,}\PY{n}{k}\PY{p}{,}\PY{n}{l}\PY{p}{,}\PY{n}{r}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
[[-0.4768 -0.9257]
 [-0.3029  1.0657]
 [ 0.767   0.0622]]
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]

        ---------------------------------------------------------------------------
    AttributeError                            Traceback (most recent call last)

        <ipython-input-24-2b56ba1db349> in <module>()
         10 \# if the input is an empty vector, it gives an error.
         11 data2 = []
    ---> 12 ScalableKMeansPlusPlus(data2,k,l,r)
    

        <ipython-input-20-16078a7eb03c> in ScalableKMeansPlusPlus(data, k, l, r)
          1 def ScalableKMeansPlusPlus(data, k, l, r):
    ----> 2     centroids = data[np.random.choice(range(data.shape[0]),1), :]
          3     data\_points = data[:, np.newaxis, :]
          4     passes = 0
          5 


        AttributeError: 'list' object has no attribute 'shape'

    \end{Verbatim}

    \subsection{\emph{Simulating data}}\label{simulating-data}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}128}]:} \PY{k}{def} \PY{n+nf}{data\PYZus{}generation}\PY{p}{(}\PY{n}{n}\PY{p}{)}\PY{p}{:}
              \PY{n}{mean1} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{9}\PY{p}{]}
              \PY{n}{cov1} \PY{o}{=} \PY{p}{[}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}
              \PY{n}{data1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{multivariate\PYZus{}normal}\PY{p}{(}\PY{n}{mean1}\PY{p}{,} \PY{n}{cov1}\PY{p}{,} \PY{n}{n}\PY{p}{)}
          
              \PY{n}{mean2} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{18}\PY{p}{,} \PY{l+m+mi}{19}\PY{p}{]}
              \PY{n}{cov2} \PY{o}{=} \PY{p}{[}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mf}{0.5}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{]}\PY{p}{]}
              \PY{n}{data2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{multivariate\PYZus{}normal}\PY{p}{(}\PY{n}{mean2}\PY{p}{,} \PY{n}{cov2}\PY{p}{,} \PY{n}{n}\PY{p}{)}
          
              \PY{n}{mean3} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{26}\PY{p}{,} \PY{l+m+mi}{27}\PY{p}{]}
              \PY{n}{cov3} \PY{o}{=} \PY{p}{[}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{]}\PY{p}{,} \PY{p}{[}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mf}{0.5}\PY{p}{]}\PY{p}{]}
              \PY{n}{data3} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{multivariate\PYZus{}normal}\PY{p}{(}\PY{n}{mean3}\PY{p}{,} \PY{n}{cov3}\PY{p}{,} \PY{n}{n}\PY{p}{)}
          
              \PY{n}{data} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{vstack}\PY{p}{(}\PY{p}{(}\PY{n}{data1}\PY{p}{,} \PY{n}{data2}\PY{p}{,} \PY{n}{data3}\PY{p}{)}\PY{p}{)}
              \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{shuffle}\PY{p}{(}\PY{n}{data}\PY{p}{)}
              \PY{k}{print} \PY{p}{(}\PY{n}{data}\PY{o}{.}\PY{n}{shape}\PY{p}{)}
              \PY{k}{return} \PY{n}{data}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}175}]:} \PY{c}{\PYZsh{}\PYZsh{}\PYZsh{} KMeans}
          
          \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{190}\PY{p}{)}
          \PY{n}{n} \PY{o}{=} \PY{l+m+mi}{100000}
          \PY{n}{k} \PY{o}{=} \PY{l+m+mi}{3}
          \PY{n}{data} \PY{o}{=} \PY{n}{data\PYZus{}generation}\PY{p}{(}\PY{n}{n}\PY{p}{)}
          
          \PY{n}{centroids} \PY{o}{=} \PY{n}{data}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{choice}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n}{data}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{,} \PY{n}{k}\PY{p}{,} \PY{n}{replace}\PY{o}{=}\PY{n+nb+bp}{False}\PY{p}{)}\PY{p}{,}\PY{p}{:}\PY{p}{]}
          
          \PY{n}{colors} \PY{o}{=} \PY{n+nb}{iter}\PY{p}{(}\PY{p}{[}\PY{l+s}{\PYZsq{}}\PY{l+s}{r}\PY{l+s}{\PYZsq{}}\PY{p}{,}\PY{l+s}{\PYZsq{}}\PY{l+s}{b}\PY{l+s}{\PYZsq{}}\PY{p}{,}\PY{l+s}{\PYZsq{}}\PY{l+s}{g}\PY{l+s}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
          \PY{n}{result1} \PY{o}{=} \PY{n}{KMeans}\PY{p}{(}\PY{n}{data}\PY{p}{,}\PY{n}{centroids}\PY{p}{,} \PY{n}{k}\PY{p}{)}
          \PY{n}{centroids1} \PY{o}{=} \PY{n}{result1}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}
          \PY{n}{min\PYZus{}dist1} \PY{o}{=} \PY{n}{result1}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}
          
          \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range} \PY{p}{(}\PY{n}{k}\PY{p}{)}\PY{p}{:}
              \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{n}{min\PYZus{}dist1}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{i}\PY{p}{]} \PY{o}{==} \PY{n+nb+bp}{True}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{data}\PY{p}{[}\PY{n}{min\PYZus{}dist1}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{i}\PY{p}{]} \PY{o}{==} \PY{n+nb+bp}{True}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{n+nb}{next}\PY{p}{(}\PY{n}{colors}\PY{p}{)}\PY{p}{)}
          \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{k}\PY{p}{)}\PY{p}{:}
              \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{centroids1}\PY{p}{[}\PY{n}{j}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n}{centroids1}\PY{p}{[}\PY{n}{j}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{color}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{w}\PY{l+s}{\PYZsq{}}\PY{p}{,}\PY{n}{marker}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{x}\PY{l+s}{\PYZsq{}}\PY{p}{)}    
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
(300000, 2)
(5, 'iterations are required to converge.')
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{MINJUNG_PARK_FINAL_PROJECT_files/MINJUNG_PARK_FINAL_PROJECT_13_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}174}]:} \PY{c}{\PYZsh{}\PYZsh{}\PYZsh{} KMeansPlusPlus}
          \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{190}\PY{p}{)}
          \PY{n}{n} \PY{o}{=} \PY{l+m+mi}{100000}
          \PY{n}{k} \PY{o}{=} \PY{l+m+mi}{3}
          \PY{n}{data} \PY{o}{=} \PY{n}{data\PYZus{}generation}\PY{p}{(}\PY{n}{n}\PY{p}{)}
          
          \PY{n}{colors} \PY{o}{=} \PY{n+nb}{iter}\PY{p}{(}\PY{p}{[}\PY{l+s}{\PYZsq{}}\PY{l+s}{r}\PY{l+s}{\PYZsq{}}\PY{p}{,}\PY{l+s}{\PYZsq{}}\PY{l+s}{b}\PY{l+s}{\PYZsq{}}\PY{p}{,}\PY{l+s}{\PYZsq{}}\PY{l+s}{g}\PY{l+s}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
          \PY{n}{result2} \PY{o}{=} \PY{n}{KMeans}\PY{p}{(}\PY{n}{data}\PY{p}{,}\PY{n}{KMeansPlusPlus}\PY{p}{(}\PY{n}{data}\PY{p}{,}\PY{n}{k}\PY{p}{)}\PY{p}{,}\PY{n}{k}\PY{p}{)}
          \PY{n}{centroids2} \PY{o}{=} \PY{n}{result2}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}
          \PY{n}{min\PYZus{}dist2} \PY{o}{=} \PY{n}{result2}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}
          
          \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range} \PY{p}{(}\PY{n}{k}\PY{p}{)}\PY{p}{:}
              \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{n}{min\PYZus{}dist2}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{i}\PY{p}{]} \PY{o}{==} \PY{n+nb+bp}{True}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{data}\PY{p}{[}\PY{n}{min\PYZus{}dist2}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{i}\PY{p}{]} \PY{o}{==} \PY{n+nb+bp}{True}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{n+nb}{next}\PY{p}{(}\PY{n}{colors}\PY{p}{)}\PY{p}{)}
          \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{k}\PY{p}{)}\PY{p}{:}
              \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{centroids2}\PY{p}{[}\PY{n}{j}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n}{centroids2}\PY{p}{[}\PY{n}{j}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{color}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{w}\PY{l+s}{\PYZsq{}}\PY{p}{,}\PY{n}{marker}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{x}\PY{l+s}{\PYZsq{}}\PY{p}{)}  
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
(300000, 2)
(3, 'iterations are required to converge.')
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{MINJUNG_PARK_FINAL_PROJECT_files/MINJUNG_PARK_FINAL_PROJECT_14_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}173}]:} \PY{c}{\PYZsh{} Scalable KMeansPlusPlus}
          \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{190}\PY{p}{)}
          \PY{n}{n} \PY{o}{=} \PY{l+m+mi}{100000}
          \PY{n}{k} \PY{o}{=} \PY{l+m+mi}{3}
          \PY{n}{data} \PY{o}{=} \PY{n}{data\PYZus{}generation}\PY{p}{(}\PY{n}{n}\PY{p}{)}
          
          \PY{n}{colors} \PY{o}{=} \PY{n+nb}{iter}\PY{p}{(}\PY{p}{[}\PY{l+s}{\PYZsq{}}\PY{l+s}{r}\PY{l+s}{\PYZsq{}}\PY{p}{,}\PY{l+s}{\PYZsq{}}\PY{l+s}{b}\PY{l+s}{\PYZsq{}}\PY{p}{,}\PY{l+s}{\PYZsq{}}\PY{l+s}{g}\PY{l+s}{\PYZsq{}}\PY{p}{]}\PY{p}{)}
          \PY{n}{result3} \PY{o}{=} \PY{n}{KMeans}\PY{p}{(}\PY{n}{data}\PY{p}{,}\PY{n}{ScalableKMeansPlusPlus}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{k}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}\PY{n}{k}\PY{p}{)}
          \PY{n}{centroids3} \PY{o}{=} \PY{n}{result3}\PY{p}{[}\PY{l+m+mi}{2}\PY{p}{]}
          \PY{n}{min\PYZus{}dist3} \PY{o}{=} \PY{n}{result3}\PY{p}{[}\PY{l+m+mi}{3}\PY{p}{]}
          \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range} \PY{p}{(}\PY{n}{k}\PY{p}{)}\PY{p}{:}
              \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{n}{min\PYZus{}dist3}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{i}\PY{p}{]} \PY{o}{==} \PY{n+nb+bp}{True}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{data}\PY{p}{[}\PY{n}{min\PYZus{}dist3}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{n}{i}\PY{p}{]} \PY{o}{==} \PY{n+nb+bp}{True}\PY{p}{,} \PY{p}{:}\PY{p}{]}\PY{p}{[}\PY{p}{:}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{n+nb}{next}\PY{p}{(}\PY{n}{colors}\PY{p}{)}\PY{p}{)}
          \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{k}\PY{p}{)}\PY{p}{:}
              \PY{n}{plt}\PY{o}{.}\PY{n}{scatter}\PY{p}{(}\PY{n}{centroids3}\PY{p}{[}\PY{n}{j}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,}\PY{n}{centroids3}\PY{p}{[}\PY{n}{j}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,}\PY{n}{color}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{w}\PY{l+s}{\PYZsq{}}\PY{p}{,}\PY{n}{marker}\PY{o}{=}\PY{l+s}{\PYZsq{}}\PY{l+s}{x}\PY{l+s}{\PYZsq{}}\PY{p}{)} 
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
(300000, 2)
(2, 'iterations are required to converge.')
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{MINJUNG_PARK_FINAL_PROJECT_files/MINJUNG_PARK_FINAL_PROJECT_15_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \subsection{\emph{Comparison}}\label{comparison}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}176}]:} \PY{n}{iteration} \PY{o}{=} \PY{n}{result1}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
          \PY{k}{print}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{iteration = }\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{n}{iteration}\PY{p}{)}
          \PY{n}{result1}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
('iteration = ', 5)
    \end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}176}]:} 16444739.6287
\end{Verbatim}
        
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}177}]:} \PY{n}{iteration} \PY{o}{=} \PY{n}{result2}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
          \PY{k}{print}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{iteration = }\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{n}{iteration}\PY{p}{)}
          \PY{n}{result2}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
('iteration = ', 3)
    \end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}177}]:} 918210.2055
\end{Verbatim}
        
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}178}]:} \PY{n}{iteration} \PY{o}{=} \PY{n}{result3}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
          \PY{k}{print}\PY{p}{(}\PY{l+s}{\PYZsq{}}\PY{l+s}{iteration = }\PY{l+s}{\PYZsq{}}\PY{p}{,} \PY{n}{iteration}\PY{p}{)}
          \PY{n}{result3}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
('iteration = ', 2)
    \end{Verbatim}

            \begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}178}]:} 1194579.1451
\end{Verbatim}
        
    According to the above three results, we can conclude that the number of
iterations in ScalableKMeansPlusPlus algorithm is the smallest one. It
means that algorithm3 is much more efficient than other algorithms.
Also, the first cluster\_values of ScalableKMeansPlusPlus is the highest
one. It means that initial centroids chosen by ScalableKMeansPlusPlus is
more efficient than others.

    \subsection{\emph{Profiling}}\label{profiling}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}179}]:} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{l+m+mi}{190}\PY{p}{)}
          \PY{n}{n} \PY{o}{=} \PY{l+m+mi}{100000}
          \PY{n}{k} \PY{o}{=} \PY{l+m+mi}{3}
          \PY{n}{data} \PY{o}{=} \PY{n}{data\PYZus{}generation}\PY{p}{(}\PY{n}{n}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
(300000, 2)
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}180}]:} \PY{o}{!} pip install \PYZhy{}\PYZhy{}pre line\PYZhy{}profiler \PYZam{}\PYZgt{} /dev/null
          \PY{o}{!} pip install psutil \PYZam{}\PYZgt{} /dev/null
          \PY{o}{!} pip install memory\PYZus{}profiler \PYZam{}\PYZgt{} /dev/null
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}181}]:} \PY{o}{\PYZpc{}}\PY{k}{load\PYZus{}ext} \PY{n}{line\PYZus{}profiler}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
The line\_profiler extension is already loaded. To reload it, use:
  \%reload\_ext line\_profiler
    \end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}182}]:} \PY{o}{\PYZpc{}}\PY{k}{lprun} \PY{o}{\PYZhy{}}\PY{n}{f} \PY{n}{KMeans} \PY{n}{KMeans}\PY{p}{(}\PY{n}{data}\PY{p}{,}\PY{n}{KMeansPlusPlus}\PY{p}{(}\PY{n}{data}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
(3, 'iterations are required to converge.')
    \end{Verbatim}

    Looking at the result of profiling for KMeansPlusPlus, this algorithm
spent most time on the clustering part.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}183}]:} \PY{o}{\PYZpc{}}\PY{k}{lprun} \PY{o}{\PYZhy{}}\PY{n}{f} \PY{n}{KMeans} \PY{n}{KMeans}\PY{p}{(}\PY{n}{data}\PY{p}{,}\PY{n}{ScalableKMeansPlusPlus}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{,} \PY{l+m+mi}{2}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{3}\PY{p}{)}
\end{Verbatim}

    \begin{Verbatim}[commandchars=\\\{\}]
(10, 'iterations are required to converge.')
    \end{Verbatim}

    ScalableMeansPlusPlus algorithm also spent most time on the clustering
part.


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
